{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)"
      ],
      "metadata": {
        "id": "mcR0yDqreDY7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50e94914-d58b-4b08-e708-5df5641ad611"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Notebook 04- Experiments & Alternative Models\n",
        "!pip install -q timm==0.9.2\n",
        "!pip install -q torchinfo\n",
        "!pip install -q einops\n",
        "\n",
        "import os, random, time, json, shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "\n",
        "import timm  #for our efficientnet and optional ViT model\n",
        "print(\"timm version:\", timm.__version__)\n",
        "print(\"Torch:\", torch.__version__, \"CUDA:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "_rLs1rAfapJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b4e69d5-6fa7-4655-db74-a7c312c73699"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/68.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25htimm version: 0.9.2\n",
            "Torch: 2.9.0+cu126 CUDA: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2- Config: adjust these\n",
        "ROOT=\"/content/drive/MyDrive/food-10/food-10\"\n",
        "CSV_DIR=os.path.join(ROOT, \"prepared_splits\")\n",
        "OUT_DIR=\"/content/food10_experiments\"\n",
        "DRIVE_OUT_DIR=os.path.join(ROOT,\"outputs_experiments\")\n",
        "os.makedirs(OUT_DIR,exist_ok=True)\n",
        "os.makedirs(DRIVE_OUT_DIR,exist_ok=True)\n",
        "\n",
        "SEED=42\n",
        "def set_seed(seed=SEED):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "set_seed()\n",
        "\n",
        "CFG={\n",
        "    \"img_size\": 224,\n",
        "    \"batch_size\": 32,      #we can reduce it if there is OOM\n",
        "    \"epochs\": 12,           #baseline quick; we can increase to 12-20 for final runs\n",
        "    \"lr\": 3e-4,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"num_workers\": min(8, os.cpu_count() or 4),\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"save_dir\": OUT_DIR,\n",
        "    \"drive_out\": DRIVE_OUT_DIR,\n",
        "    \"mixed_precision\": True\n",
        "}\n",
        "print(\"Device:\", CFG['device'], \"Out dir:\", OUT_DIR)"
      ],
      "metadata": {
        "id": "UYzdYkeZarVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fac1fd9-c961-408b-b94f-537b8c444b77"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda Out dir: /content/food10_experiments\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 3- load prepared CSVs\n",
        "train_df=pd.read_csv(os.path.join(CSV_DIR,\"train.csv\"))\n",
        "val_df=pd.read_csv(os.path.join(CSV_DIR,\"val.csv\"))\n",
        "print(\"Train rows:\",len(train_df),\"Val rows:\",len(val_df))\n",
        "print(\"Columns:\",train_df.columns.tolist())\n",
        "assert 'fullpath' in train_df.columns and 'label' in train_df.columns, \"train.csv must include fullpath and label\"\n",
        "classes = sorted(train_df['class'].unique())\n",
        "num_classes = len(classes)\n",
        "print(\"Num classes:\", num_classes)"
      ],
      "metadata": {
        "id": "EdG_SddtasL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc75de57-a56a-4213-e922-8ed1b4c0de40"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train rows: 6000 Val rows: 1500\n",
            "Columns: ['path', 'class', 'fullpath', 'label']\n",
            "Num classes: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 4- Transforms & dataset\n",
        "IMG_SIZE=CFG['img_size']\n",
        "train_transforms=transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMG_SIZE),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(0.2,0.2,0.2,0.05),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "val_transforms=transforms.Compose([\n",
        "    transforms.Resize(int(IMG_SIZE*1.15)),\n",
        "    transforms.CenterCrop(IMG_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "class Food10Dataset(Dataset):\n",
        "    def __init__(self,df,transform=None):\n",
        "        self.df=df.reset_index(drop=True)\n",
        "        self.transform=transform\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self,idx):\n",
        "        r=self.df.iloc[idx]\n",
        "        img=Image.open(r['fullpath']).convert('RGB')\n",
        "        if self.transform:\n",
        "            img=self.transform(img)\n",
        "        label=int(r['label'])\n",
        "        return img,label"
      ],
      "metadata": {
        "id": "aRuhAe_oauOM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 5- Model factory that returns a model and parameter groups (for freezing)\n",
        "def get_model(name,num_classes,pretrained=True):\n",
        "    name=name.lower()\n",
        "    if name==\"resnet101\":\n",
        "        model=timm.create_model('resnet101',pretrained=pretrained,num_classes=num_classes)\n",
        "    elif name==\"resnet50\":\n",
        "        model=timm.create_model('resnet50',pretrained=pretrained,num_classes=num_classes)\n",
        "    elif name==\"efficientnet_b0\" or name==\"efficientnet-b0\":\n",
        "        #timm name: efficientnet_b0\n",
        "        model=timm.create_model('efficientnet_b0',pretrained=pretrained,num_classes=num_classes)\n",
        "    #elif name == \"vit_b16\" or name == \"vit_b_16\":\n",
        "        #optional: ViT-B/16 (may be slow on Colab)\n",
        "        #model=timm.create_model('vit_base_patch16_224', pretrained=pretrained, num_classes=num_classes)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model: \" + name)\n",
        "    return model\n",
        "#Quick smoke test:\n",
        "#m=get_model(\"efficientnet_b0\", num_classes=10); print(m)"
      ],
      "metadata": {
        "id": "jytc5lTEawcR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 6- training helpers\n",
        "from sklearn.metrics import f1_score\n",
        "device=torch.device(CFG['device'])\n",
        "def train_one_epoch(model, loader, optimizer, criterion, scaler, device):\n",
        "    model.train()\n",
        "    losses=[]; preds=[]; targets=[]\n",
        "    loop=tqdm(loader,desc=\"Train\",leave=False)\n",
        "    for imgs,lbls in loop:\n",
        "        imgs=imgs.to(device,non_blocking=True)\n",
        "        lbls=lbls.to(device,non_blocking=True)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.amp.autocast(device_type='cuda',enabled=CFG['mixed_precision'] and device.type=='cuda'):\n",
        "            out=model(imgs)\n",
        "            loss=criterion(out, lbls)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        losses.append(loss.item())\n",
        "        preds.extend(out.argmax(dim=1).cpu().numpy().tolist())\n",
        "        targets.extend(lbls.cpu().numpy().tolist())\n",
        "        loop.set_postfix(loss=np.mean(losses))\n",
        "    return np.mean(losses),f1_score(targets,preds,average='macro')\n",
        "\n",
        "def validate(model,loader,criterion,device):\n",
        "    model.eval()\n",
        "    losses=[]; preds=[]; targets=[]\n",
        "    with torch.no_grad():\n",
        "        loop=tqdm(loader,desc=\"Val  \",leave=False)\n",
        "        for imgs, lbls in loop:\n",
        "            imgs=imgs.to(device,non_blocking=True)\n",
        "            lbls=lbls.to(device,non_blocking=True)\n",
        "            out=model(imgs)\n",
        "            loss=criterion(out,lbls)\n",
        "            losses.append(loss.item())\n",
        "            preds.extend(out.argmax(dim=1).cpu().numpy().tolist())\n",
        "            targets.extend(lbls.cpu().numpy().tolist())\n",
        "    return np.mean(losses),f1_score(targets,preds,average='macro'),targets,preds"
      ],
      "metadata": {
        "id": "uvsFxt1jayYj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 7- run an experiment with a given model name and options\n",
        "def run_experiment(model_name, train_df, val_df, cfg=CFG, freeze_backbone=False, head_epochs=1, run_name=None):\n",
        "    if run_name is None:\n",
        "        run_name=f\"{model_name}_freeze{freeze_backbone}\"\n",
        "    print(\"\\n=== Running:\", run_name, \"===\")\n",
        "    num_classes=int(train_df['label'].nunique())\n",
        "    model=get_model(model_name, num_classes=num_classes, pretrained=True)\n",
        "    model=model.to(device)\n",
        "\n",
        "    if freeze_backbone:\n",
        "        #freeze all except classifier head\n",
        "        for name,param in model.named_parameters():\n",
        "            param.requires_grad=False\n",
        "        #identify head parameters-timm models differ; set last fc/head trainable\n",
        "        if hasattr(model,'fc'):\n",
        "            for p in model.fc.parameters():\n",
        "                p.requires_grad=True\n",
        "        elif hasattr(model,'classifier'):\n",
        "            for p in model.classifier.parameters():\n",
        "                p.requires_grad=True\n",
        "        elif hasattr(model,'head'):\n",
        "            for p in model.head.parameters():\n",
        "                p.requires_grad=True\n",
        "\n",
        "    #Prepare dataloaders\n",
        "    train_ds=Food10Dataset(train_df,transform=train_transforms)\n",
        "    val_ds=Food10Dataset(val_df,transform=val_transforms)\n",
        "    train_loader=DataLoader(train_ds,batch_size=cfg['batch_size'],shuffle=True,\n",
        "                              num_workers=cfg['num_workers'],pin_memory=(cfg['device']=='cuda'))\n",
        "    val_loader=DataLoader(val_ds, batch_size=cfg['batch_size'],shuffle=False,\n",
        "                            num_workers=cfg['num_workers'],pin_memory=(cfg['device']=='cuda'))\n",
        "\n",
        "    #optimizer uses only trainable params\n",
        "    optimizer=optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),lr=cfg['lr'],weight_decay=cfg['weight_decay'])\n",
        "    scheduler=optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg['epochs'])\n",
        "    criterion=nn.CrossEntropyLoss()\n",
        "    scaler=torch.cuda.amp.GradScaler(enabled=(cfg['mixed_precision'] and device.type=='cuda'))\n",
        "    best_f1=0.0\n",
        "    history={\"train_loss\":[],\"train_f1\":[],\"val_loss\":[],\"val_f1\":[]}\n",
        "    best_path=os.path.join(cfg['save_dir'],f\"best_{run_name}.pth\")\n",
        "    #Optionally run head-only epochs first\n",
        "    start_epoch=0\n",
        "    if freeze_backbone and head_epochs>0:\n",
        "        print(\"Training head-only for\",head_epochs,\"epochs\")\n",
        "        for e in range(head_epochs):\n",
        "            tr_loss,tr_f1=train_one_epoch(model,train_loader,optimizer,criterion,scaler,device)\n",
        "            val_loss,val_f1,_,_=validate(model,val_loader,criterion,device)\n",
        "            scheduler.step()\n",
        "            history['train_loss'].append(tr_loss); history['train_f1'].append(tr_f1)\n",
        "            history['val_loss'].append(val_loss); history['val_f1'].append(val_f1)\n",
        "            print(f\"Head Epoch {e+1} train_f1 {tr_f1:.4f} val_f1 {val_f1:.4f}\")\n",
        "            if val_f1>best_f1:\n",
        "                best_f1=val_f1\n",
        "                torch.save({\"model_state\":model.state_dict(),\"cfg\":cfg,\"epoch\":e},best_path)\n",
        "        #unfreeze all for full fine-tune\n",
        "        print(\"Unfreezing all parameters for fine-tuning\")\n",
        "        for p in model.parameters(): p.requires_grad=True\n",
        "        optimizer=optim.AdamW(model.parameters(),lr=cfg['lr']/3,weight_decay=cfg['weight_decay'])\n",
        "        scheduler=optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg['epochs'])\n",
        "\n",
        "    #Full training\n",
        "    for epoch in range(cfg['epochs']):\n",
        "        print(f\"\\nEpoch {epoch+1}/{cfg['epochs']}\")\n",
        "        tr_loss,tr_f1=train_one_epoch(model,train_loader,optimizer,criterion,scaler,device)\n",
        "        val_loss,val_f1,val_targets,val_preds=validate(model,val_loader,criterion,device)\n",
        "        scheduler.step()\n",
        "        history['train_loss'].append(tr_loss); history['train_f1'].append(tr_f1)\n",
        "        history['val_loss'].append(val_loss); history['val_f1'].append(val_f1)\n",
        "        print(f\"Epoch {epoch+1} Train loss {tr_loss:.4f} f1 {tr_f1:.4f} | Val loss {val_loss:.4f} f1 {val_f1:.4f}\")\n",
        "\n",
        "        if val_f1 > best_f1:\n",
        "            best_f1=val_f1\n",
        "            ckpt={\"model_state\":model.state_dict(),\"optimizer_state\":optimizer.state_dict(),\"cfg\":cfg,\"epoch\":epoch,\"best_f1\":best_f1}\n",
        "            torch.save(ckpt, best_path)\n",
        "            shutil.copy(best_path, os.path.join(cfg['drive_out'], os.path.basename(best_path)))\n",
        "            print(\"Saved best model:\", best_path)\n",
        "        #save history after each epoch\n",
        "        with open(os.path.join(cfg['save_dir'], f\"history_{run_name}.json\"), \"w\") as f:\n",
        "            json.dump(history, f)\n",
        "    print(\"Finished experiment:\", run_name, \"best_val_f1:\", best_f1)\n",
        "    return {\"name\":run_name, \"best_val_f1\":best_f1, \"history\":history, \"best_path\":best_path}"
      ],
      "metadata": {
        "id": "ELo2MmwAiEjX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 8- run a couple of experiments and collect results\n",
        "experiments=[]\n",
        "#1) EfficientNet-B0: freeze head 1 epoch then full fine-tune\n",
        "experiments.append(run_experiment(\"efficientnet_b0\", train_df, val_df, cfg=CFG, freeze_backbone=True, head_epochs=1, run_name=\"effnetb0_freeze1\"))\n",
        "#2) ResNet-101: no freeze, full fine-tune\n",
        "experiments.append(run_experiment(\"resnet101\", train_df, val_df, cfg=CFG, freeze_backbone=False, head_epochs=0, run_name=\"resnet101_full\"))\n",
        "#3) Optional: ViT-B/16 (uncomment if you want)\n",
        "#experiments.append(run_experiment(\"vit_b16\", train_df, val_df, cfg=CFG, freeze_backbone=False, head_epochs=0, run_name=\"vitb16_full\"))\n",
        "#Save experiment summary table\n",
        "exp_df=pd.DataFrame([{\"name\":e['name'],\"best_val_f1\":e['best_val_f1'],\"best_path\":e['best_path']} for e in experiments])\n",
        "exp_df.to_csv(os.path.join(CFG['save_dir'], \"experiment_summary.csv\"), index=False)\n",
        "print(\"Experiments finished. Summary:\")\n",
        "print(exp_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "KG9PcawLjw5l",
        "outputId": "4824d27e-6bbe-4913-f28c-cedc0c2b12f4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'run_experiment' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-424450820.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mexperiments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#1) EfficientNet-B0: freeze head 1 epoch then full fine-tune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mexperiments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"efficientnet_b0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreeze_backbone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"effnetb0_freeze1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#2) ResNet-101: no freeze, full fine-tune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mexperiments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resnet101\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreeze_backbone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"resnet101_full\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'run_experiment' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 9- Plot results from histories\n",
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n",
        "#collect histories\n",
        "for e in experiments:\n",
        "    h=e['history']\n",
        "    epochs=list(range(1, len(h['val_f1'])+1))\n",
        "    plt.plot(epochs,h['val_f1'],marker='o',label=e['name'])\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Val Macro F1\"); plt.title(\"Val Macro F1- experiments\")\n",
        "plt.legend(); plt.grid(True)\n",
        "plt.savefig(os.path.join(CFG['save_dir'], \"experiments_comparison_val_f1.png\"), dpi=150)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zYTuXme_kVoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 10- Evaluate experiment best checkpoints on the untouched test set\n",
        "#This uses the same robust matching used earlier to build test_df from test.txt\n",
        "def build_test_df(ROOT):\n",
        "    rows=[]\n",
        "    IMAGES_DIR=os.path.join(ROOT,\"images\")\n",
        "    for cls in sorted(os.listdir(IMAGES_DIR)):\n",
        "        cls_dir=os.path.join(IMAGES_DIR,cls)\n",
        "        if not os.path.isdir(cls_dir): continue\n",
        "        for fname in os.listdir(cls_dir):\n",
        "            if fname.lower().endswith(('.jpg','.jpeg','.png')):\n",
        "                rows.append({\"path\":f\"{cls}/{fname}\",\"class\": cls,\"fullpath\":os.path.join(cls_dir,fname)})\n",
        "    df_all=pd.DataFrame(rows)\n",
        "    df_all['no_ext']=df_all['class']+\"/\"+df_all['path'].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
        "    test_txt=os.path.join(ROOT,\"test.txt\")\n",
        "    with open(test_txt,'r') as f: tlines=[l.strip() for l in f if l.strip()]\n",
        "    matched=[]; missing=[]\n",
        "    for e in tlines:\n",
        "        if e in df_all['path'].values: matched.append(df_all[df_all['path']==e].iloc[0])\n",
        "        elif e in df_all['no_ext'].values: matched.append(df_all[df_all['no_ext']==e].iloc[0])\n",
        "        else:\n",
        "            base=os.path.basename(e).split('.')[0]\n",
        "            cand=df_all[df_all['path'].str.contains(base)]\n",
        "            if len(cand)==1: matched.append(cand.iloc[0])\n",
        "            else: missing.append(e)\n",
        "    if missing:\n",
        "        print(\"Warning: missing test matches (sample):\",missing[:10])\n",
        "    return pd.DataFrame(matched).reset_index(drop=True),df_all\n",
        "test_df,df_all=build_test_df(ROOT)\n",
        "print(\"Test rows:\",len(test_df))\n",
        "def eval_checkpoint(pth,df_all,test_df):\n",
        "    ckpt=torch.load(pth,map_location='cpu')\n",
        "    #infer num classes from ckpt\n",
        "    ms=ckpt.get('model_state',ckpt)\n",
        "    fc_key=None\n",
        "    for k in ms.keys():\n",
        "        if k.endswith('fc.weight') or '.fc.weight' in k:\n",
        "            fc_key=k; break\n",
        "    num_classes_ckpt=ms[fc_key].shape[0]\n",
        "    model=timm.create_model('resnet50',pretrained=False,num_classes=num_classes_ckpt)  #placeholder\n",
        "    #we need to create a model of the same arch- naive approach: load the model from ckpt if cfg saved\n",
        "    #For safety, try to detect model name inside ckpt['cfg'] if present\n",
        "    model_name=ckpt.get('cfg',{}).get('model_name',None)\n",
        "    if model_name:\n",
        "        print(\"Detected model in checkpoint cfg:\",model_name)\n",
        "        model=get_model(model_name,num_classes_ckpt,pretrained=False)\n",
        "    else:\n",
        "        #fallback to resnet50-sized architecture\n",
        "        model=get_model('resnet50',num_classes_ckpt,pretrained=False)\n",
        "    model.load_state_dict(ckpt['model_state'])\n",
        "    model.to(device).eval()\n",
        "    val_tf=transforms.Compose([transforms.Resize(int(IMG_SIZE*1.15)),transforms.CenterCrop(IMG_SIZE),\n",
        "                                 transforms.ToTensor(),transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])])\n",
        "    all_preds=[]; all_trues=[]\n",
        "    classes_sorted=sorted(df_all['class'].unique())\n",
        "    batch=64\n",
        "    for i in range(0,len(test_df),batch):\n",
        "        batch_df=test_df.iloc[i:i+batch]\n",
        "        imgs=[]\n",
        "        for p in batch_df['fullpath'].tolist():\n",
        "            imgs.append(val_tf(Image.open(p).convert('RGB')))\n",
        "        x=torch.stack(imgs).to(device)\n",
        "        with torch.no_grad():\n",
        "            out=model(x)\n",
        "            preds=out.argmax(dim=1).cpu().numpy().tolist()\n",
        "        all_preds.extend(preds)\n",
        "        all_trues.extend([int(train_df[train_df['class']==c]['label'].iloc[0]) for c in batch_df['class'].tolist()])\n",
        "    test_f1=f1_score(all_trues, all_preds, average='macro')\n",
        "    print(\"Eval\",os.path.basename(pth),\"Test Macro F1:\",test_f1)\n",
        "    return test_f1\n",
        "#Evaluate best checkpoints saved by experiments\n",
        "for row in pd.read_csv(os.path.join(CFG['save_dir'],\"experiment_summary.csv\")).to_dict('records'):\n",
        "    pth=row['best_path']\n",
        "    if os.path.exists(pth):\n",
        "        try:\n",
        "            _=eval_checkpoint(pth,df_all,test_df)\n",
        "        except Exception as e:\n",
        "            print(\"Evaluation failed for\",pth,\":\",e)\n",
        "    else:\n",
        "        print(\"Checkpoint not found:\",pth)"
      ],
      "metadata": {
        "id": "DLYVqVmctfsJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}