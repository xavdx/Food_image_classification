{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#1) Unmount then remount (force prompt to pick account)\n",
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGtCti6TWc4A",
        "outputId": "c571d992-c07b-430c-981e-c017c93d83bd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "print(\"List top level MyDrive:\")\n",
        "print(os.listdir('/content/drive/MyDrive')[:100])   # quick peek\n",
        "\n",
        "# Search for 'food-10' directory and any .pth in mounted drive\n",
        "print(\"Searching for food-10 folders...\")\n",
        "print(glob.glob('/content/drive/MyDrive/**/food-10*', recursive=True)[:50])\n",
        "print(\"Searching for .pth checkpoints (this may take a moment)...\")\n",
        "print(glob.glob('/content/drive/MyDrive/**/*.pth', recursive=True)[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6Zlj1QiWjKL",
        "outputId": "6ed8799e-7b10-4a29-f19e-9ee0e0a52518"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List top level MyDrive:\n",
            "['Classroom', 'AD&ML Project- Construction Website', 'tik-qmcv-hkh â€“ 31 May 2024.pdf', 'Colab Notebooks', 'food_project', 'food-10']\n",
            "Searching for food-10 folders...\n",
            "['/content/drive/MyDrive/food-10', '/content/drive/MyDrive/food-10/food-10']\n",
            "Searching for .pth checkpoints (this may take a moment)...\n",
            "['/content/drive/MyDrive/food-10/food-10/outputs/best_resnet50.pth', '/content/drive/MyDrive/food-10/food-10/outputs_experiments/best_effnetb0_freeze1.pth', '/content/drive/MyDrive/food-10/food-10/outputs_experiments/best_resnet101_full.pth']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_path=\"/content/drive/MyDrive/food-10/food-10/outputs_experiments/best_effnetb0_freeze1.pth\"\n",
        "import os\n",
        "print(\"Exists:\",os.path.exists(ckpt_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChcXZ2xhXMlF",
        "outputId": "d1b46af6-4d86-49e2-f1a3-871bb26b5c00"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, os\n",
        "local_out=\"/content/food10_outputs\"\n",
        "os.makedirs(local_out,exist_ok=True)\n",
        "shutil.copy(ckpt_path,os.path.join(local_out,os.path.basename(ckpt_path)))\n",
        "print(\"Copied to\",local_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpjN-3JBXRYd",
        "outputId": "6524c010-f898-4f78-f478-8d32984c628d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied to /content/food10_outputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, glob, json, os\n",
        "ckpts = glob.glob(\"/content/drive/MyDrive/food-10/food-10/**/best_*.pth\", recursive=True)\n",
        "print(\"Found checkpoints:\", ckpts)\n",
        "\n",
        "for p in ckpts:\n",
        "    print(\"\\n---\", p)\n",
        "    ck = torch.load(p, map_location='cpu')\n",
        "    print(\"Keys in checkpoint:\", list(ck.keys()))\n",
        "    print(\"Epoch in ckpt:\", ck.get('epoch', 'N/A'))\n",
        "    print(\"Best val f1 in ckpt:\", ck.get('best_f1', ck.get('best_val_f1', 'N/A')))\n",
        "    if 'cfg' in ck:\n",
        "        print(\"cfg.model_name:\", ck['cfg'].get('model_name', None))\n",
        "# History JSONs\n",
        "hist_files = glob.glob(\"/content/drive/MyDrive/food-10/food-10/**/history*.json\", recursive=True)\n",
        "print(\"\\nFound history files:\", hist_files)\n",
        "for h in hist_files:\n",
        "    print(\"History:\", h)\n",
        "    try:\n",
        "        with open(h,'r') as f:\n",
        "            hist = json.load(f)\n",
        "        print(\"history keys:\", hist.keys())\n",
        "        # print last epoch stats (if present)\n",
        "        valf = hist.get('val_f1', [])\n",
        "        if valf:\n",
        "            print(\"last val_f1:\", valf[-1], \"best val_f1:\", max(valf))\n",
        "    except Exception as e:\n",
        "        print(\"Could not read history:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uN4xwBOSb9-f",
        "outputId": "ae21c7e4-eaee-47f9-a12f-f65fad50a147"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found checkpoints: ['/content/drive/MyDrive/food-10/food-10/outputs/best_resnet50.pth', '/content/drive/MyDrive/food-10/food-10/outputs_experiments/best_effnetb0_freeze1.pth', '/content/drive/MyDrive/food-10/food-10/outputs_experiments/best_resnet101_full.pth']\n",
            "\n",
            "--- /content/drive/MyDrive/food-10/food-10/outputs/best_resnet50.pth\n",
            "Keys in checkpoint: ['model_state', 'optimizer_state', 'scheduler_state', 'epoch', 'best_f1', 'cfg']\n",
            "Epoch in ckpt: 10\n",
            "Best val f1 in ckpt: 0.9023548931635321\n",
            "cfg.model_name: resnet50\n",
            "\n",
            "--- /content/drive/MyDrive/food-10/food-10/outputs_experiments/best_effnetb0_freeze1.pth\n",
            "Keys in checkpoint: ['model_state', 'optimizer_state', 'cfg', 'epoch', 'best_f1']\n",
            "Epoch in ckpt: 11\n",
            "Best val f1 in ckpt: 0.8739598559535151\n",
            "cfg.model_name: None\n",
            "\n",
            "--- /content/drive/MyDrive/food-10/food-10/outputs_experiments/best_resnet101_full.pth\n",
            "Keys in checkpoint: ['model_state', 'optimizer_state', 'cfg', 'epoch', 'best_f1']\n",
            "Epoch in ckpt: 6\n",
            "Best val f1 in ckpt: 0.9046313937669149\n",
            "cfg.model_name: None\n",
            "\n",
            "Found history files: ['/content/drive/MyDrive/food-10/food-10/outputs/history.json']\n",
            "History: /content/drive/MyDrive/food-10/food-10/outputs/history.json\n",
            "history keys: dict_keys(['train_loss', 'train_f1', 'val_loss', 'val_f1'])\n",
            "last val_f1: 0.9008959008963757 best val_f1: 0.9023548931635321\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_path = \"/content/drive/MyDrive/food-10/food-10/outputs_experiments/best_resnet101_full.pth\"\n",
        "ck = torch.load(ckpt_path, map_location='cpu')\n",
        "\n",
        "# infer num_classes\n",
        "ms = ck['model_state']\n",
        "fc_key = next(k for k in ms.keys() if 'fc.weight' in k)\n",
        "num_classes = ms[fc_key].shape[0]\n",
        "\n",
        "import timm, torch\n",
        "model = timm.create_model('resnet101', pretrained=False, num_classes=num_classes)\n",
        "model.load_state_dict(ms)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
        "optimizer.load_state_dict(ck['optimizer_state'])\n",
        "\n",
        "start_epoch = ck['epoch'] + 1\n",
        "best_f1 = ck['best_f1']\n",
        "\n",
        "print(\"Resuming from epoch:\", start_epoch, \"Best F1:\", best_f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPGTTb2MfhZU",
        "outputId": "50b7b644-fa2c-49c8-ef03-637543c6f610"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming from epoch: 7 Best F1: 0.9046313937669149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Resuming the ResNet-101 training cell\n",
        "import os, json, shutil, time\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import torch, timm\n",
        "import torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# --- Config (edit only if you want different final epoch/batch) ---\n",
        "ROOT = \"/content/drive/MyDrive/food-10/food-10\"\n",
        "SPLIT_DIR = os.path.join(ROOT, \"prepared_splits\")\n",
        "DRIVE_OUT_DIR = os.path.join(ROOT, \"outputs_experiments\")\n",
        "LOCAL_OUT = \"/content/food10_outputs\"\n",
        "os.makedirs(LOCAL_OUT, exist_ok=True)\n",
        "os.makedirs(DRIVE_OUT_DIR, exist_ok=True)\n",
        "\n",
        "TOTAL_EPOCHS = 12            # final target epochs (was 12 in your runs)\n",
        "BATCH_SIZE = 32              # keep same as before\n",
        "IMG_SIZE = 224\n",
        "LR = 3e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "NUM_WORKERS = min(8, os.cpu_count() or 4)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# --- Paths & checkpoint to resume from ---\n",
        "ckpt_path = \"/content/drive/MyDrive/food-10/food-10/outputs_experiments/best_resnet101_full.pth\"\n",
        "assert os.path.exists(ckpt_path), \"Checkpoint not found: \" + ckpt_path\n",
        "\n",
        "# --- Load checkpoint (fast step you already did) ---\n",
        "ck = torch.load(ckpt_path, map_location='cpu')\n",
        "ms = ck['model_state']\n",
        "# detect fc weight key and num_classes\n",
        "fc_key = next((k for k in ms.keys() if 'fc.weight' in k or k.endswith('fc.weight')), None)\n",
        "if fc_key is None:\n",
        "    raise RuntimeError(\"Could not find fc.weight key in checkpoint.\")\n",
        "num_classes = ms[fc_key].shape[0]\n",
        "print(\"Num classes inferred:\", num_classes)\n",
        "\n",
        "# --- Build model and load state ---\n",
        "model = timm.create_model('resnet101', pretrained=False, num_classes=num_classes)\n",
        "model.load_state_dict(ms)\n",
        "model.to(device)\n",
        "\n",
        "# --- Rebuild data pipeline (same as training) ---\n",
        "train_df = pd.read_csv(os.path.join(SPLIT_DIR, \"train.csv\"))\n",
        "val_df   = pd.read_csv(os.path.join(SPLIT_DIR, \"val.csv\"))\n",
        "assert 'fullpath' in train_df.columns and 'label' in train_df.columns\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMG_SIZE),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(0.2,0.2,0.2,0.05),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize(int(IMG_SIZE*1.15)),\n",
        "    transforms.CenterCrop(IMG_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "class Food10Dataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.df.iloc[idx]\n",
        "        img = Image.open(r['fullpath']).convert('RGB')\n",
        "        if self.transform: img = self.transform(img)\n",
        "        label = int(r['label'])\n",
        "        return img, label\n",
        "\n",
        "train_ds = Food10Dataset(train_df, transform=train_tf)\n",
        "val_ds   = Food10Dataset(val_df, transform=val_tf)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'))\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'))\n",
        "\n",
        "# --- Optimizer / scheduler / criterion / scaler ---\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "if 'optimizer_state' in ck:\n",
        "    try:\n",
        "        optimizer.load_state_dict(ck['optimizer_state'])\n",
        "        print(\"Loaded optimizer state.\")\n",
        "    except Exception as e:\n",
        "        print(\"Warning: couldn't load optimizer state:\", e)\n",
        "\n",
        "# Scheduler: try to restore if present, else create new\n",
        "scheduler = None\n",
        "if 'scheduler_state' in ck:\n",
        "    try:\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TOTAL_EPOCHS)\n",
        "        scheduler.load_state_dict(ck['scheduler_state'])\n",
        "        print(\"Loaded scheduler state.\")\n",
        "    except Exception as e:\n",
        "        print(\"Warning: couldn't load scheduler state:\", e)\n",
        "if scheduler is None:\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TOTAL_EPOCHS)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
        "\n",
        "# --- History (load & continue) ---\n",
        "history_local = {\"train_loss\":[], \"train_f1\":[], \"val_loss\":[], \"val_f1\":[]}\n",
        "# Try to find existing history file near checkpoint\n",
        "hist_candidates = []\n",
        "hist_candidates += [os.path.join(LOCAL_OUT, \"history_resnet101_resume.json\")]\n",
        "hist_candidates += list(glob.glob(os.path.join(os.path.dirname(ckpt_path), \"history_*.json\")))\n",
        "hist_candidates += list(glob.glob(os.path.join(LOCAL_OUT, \"history_*.json\")))\n",
        "hist_candidates = [p for p in hist_candidates if os.path.exists(p)]\n",
        "if hist_candidates:\n",
        "    try:\n",
        "        with open(hist_candidates[0],'r') as f:\n",
        "            history_local = json.load(f)\n",
        "        print(\"Loaded history from\", hist_candidates[0])\n",
        "    except Exception as e:\n",
        "        print(\"Couldn't load history file:\", e)\n",
        "\n",
        "best_f1 = ck.get('best_f1', 0.0)\n",
        "start_epoch = int(ck.get('epoch', -1)) + 1\n",
        "print(\"Resuming training from epoch\", start_epoch, \"best_f1 so far\", best_f1)\n",
        "\n",
        "# --- helper train/val functions (same as before) ---\n",
        "from tqdm import tqdm\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device, scaler):\n",
        "    model.train()\n",
        "    losses=[]; preds=[]; targets=[]\n",
        "    loop = tqdm(loader, desc=\"Train\", leave=False)\n",
        "    for imgs, lbls in loop:\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        lbls = lbls.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.amp.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n",
        "            out = model(imgs)\n",
        "            loss = criterion(out, lbls)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        losses.append(loss.item())\n",
        "        preds.extend(out.argmax(dim=1).cpu().numpy().tolist())\n",
        "        targets.extend(lbls.cpu().numpy().tolist())\n",
        "        loop.set_postfix(loss=np.mean(losses))\n",
        "    return float(np.mean(losses)), float(f1_score(targets, preds, average='macro'))\n",
        "\n",
        "def validate_model(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    losses=[]; preds=[]; targets=[]\n",
        "    with torch.no_grad():\n",
        "        loop = tqdm(loader, desc=\"Val\", leave=False)\n",
        "        for imgs, lbls in loop:\n",
        "            imgs = imgs.to(device, non_blocking=True)\n",
        "            lbls = lbls.to(device, non_blocking=True)\n",
        "            out = model(imgs)\n",
        "            loss = criterion(out, lbls)\n",
        "            losses.append(loss.item())\n",
        "            preds.extend(out.argmax(dim=1).cpu().numpy().tolist())\n",
        "            targets.extend(lbls.cpu().numpy().tolist())\n",
        "    return float(np.mean(losses)), float(f1_score(targets, preds, average='macro')), targets, preds\n",
        "\n",
        "# --- Resume loop ---\n",
        "best_path = os.path.join(LOCAL_OUT, \"best_resnet101_resumed.pth\")\n",
        "drive_best_path = os.path.join(DRIVE_OUT_DIR, os.path.basename(best_path))\n",
        "\n",
        "for epoch in range(start_epoch, TOTAL_EPOCHS):\n",
        "    print(f\"\\n=== Epoch {epoch+1}/{TOTAL_EPOCHS} ===\")\n",
        "    t0 = time.time()\n",
        "    tr_loss, tr_f1 = train_one_epoch(model, train_loader, optimizer, criterion, device, scaler)\n",
        "    val_loss, val_f1, val_targets, val_preds = validate_model(model, val_loader, criterion, device)\n",
        "    # step scheduler after epoch\n",
        "    try:\n",
        "        scheduler.step()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    history_local['train_loss'].append(tr_loss)\n",
        "    history_local['train_f1'].append(tr_f1)\n",
        "    history_local['val_loss'].append(val_loss)\n",
        "    history_local['val_f1'].append(val_f1)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Train loss {tr_loss:.4f} f1 {tr_f1:.4f} | Val loss {val_loss:.4f} f1 {val_f1:.4f} | time {(time.time()-t0):.1f}s\")\n",
        "\n",
        "    # save checkpoint if improved\n",
        "    if val_f1 > best_f1:\n",
        "        best_f1 = val_f1\n",
        "        ckpt = {\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"scheduler_state\": scheduler.state_dict() if hasattr(scheduler, 'state_dict') else None,\n",
        "            \"epoch\": epoch,\n",
        "            \"best_f1\": best_f1,\n",
        "            \"cfg\": {\"model_name\":\"resnet101\", \"lr\": LR}\n",
        "        }\n",
        "        torch.save(ckpt, best_path)\n",
        "        # copy to Drive\n",
        "        shutil.copy(best_path, drive_best_path)\n",
        "        print(\"Saved NEW best model with val_f1:\", best_f1)\n",
        "\n",
        "    # save periodic checkpoint (every epoch) to local and drive\n",
        "    epoch_ckpt = os.path.join(LOCAL_OUT, f\"ckpt_resnet101_epoch{epoch+1}.pth\")\n",
        "    torch.save({\"model_state\": model.state_dict(), \"optimizer_state\": optimizer.state_dict(), \"epoch\": epoch, \"best_f1\": best_f1}, epoch_ckpt)\n",
        "    try:\n",
        "        shutil.copy(epoch_ckpt, os.path.join(DRIVE_OUT_DIR, os.path.basename(epoch_ckpt)))\n",
        "    except Exception as e:\n",
        "        print(\"Warning: could not copy epoch ckpt to Drive:\", e)\n",
        "\n",
        "    # save history\n",
        "    hist_local_path = os.path.join(LOCAL_OUT, \"history_resnet101_resumed.json\")\n",
        "    with open(hist_local_path, \"w\") as f:\n",
        "        json.dump(history_local, f)\n",
        "    try:\n",
        "        shutil.copy(hist_local_path, os.path.join(DRIVE_OUT_DIR, os.path.basename(hist_local_path)))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "print(\"\\nResume training finished. Best val F1:\", best_f1)\n",
        "print(\"Best checkpoint on local:\", best_path)\n",
        "print(\"Best checkpoint on Drive:\", drive_best_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePPme-LohaMZ",
        "outputId": "3c152b95-b5be-4825-c3f6-68735b72e621"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Num classes inferred: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-940596989.py:108: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded optimizer state.\n",
            "Resuming training from epoch 7 best_f1 so far 0.9046313937669149\n",
            "\n",
            "=== Epoch 8/12 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Train loss 0.3840 f1 0.8720 | Val loss 0.3043 f1 0.9055 | time 1756.5s\n",
            "Saved NEW best model with val_f1: 0.9054960671834689\n",
            "\n",
            "=== Epoch 9/12 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Train loss 0.3413 f1 0.8876 | Val loss 0.3003 f1 0.9121 | time 97.0s\n",
            "Saved NEW best model with val_f1: 0.912115214177519\n",
            "\n",
            "=== Epoch 10/12 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Train loss 0.3393 f1 0.8867 | Val loss 0.3005 f1 0.9096 | time 108.4s\n",
            "\n",
            "=== Epoch 11/12 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 Train loss 0.3042 f1 0.9017 | Val loss 0.3068 f1 0.9076 | time 94.7s\n",
            "\n",
            "=== Epoch 12/12 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 Train loss 0.3028 f1 0.8998 | Val loss 0.3175 f1 0.9097 | time 95.8s\n",
            "\n",
            "Resume training finished. Best val F1: 0.912115214177519\n",
            "Best checkpoint on local: /content/food10_outputs/best_resnet101_resumed.pth\n",
            "Best checkpoint on Drive: /content/drive/MyDrive/food-10/food-10/outputs_experiments/best_resnet101_resumed.pth\n"
          ]
        }
      ]
    }
  ]
}